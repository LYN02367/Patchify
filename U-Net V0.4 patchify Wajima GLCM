import os
import math
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 
import cv2
import numpy as np
import rasterio
import itertools
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from patchify import patchify, unpatchify
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.preprocessing.image import save_img
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Dropout
from osgeo import gdal, osr
from rasterio.warp import calculate_default_transform, reproject, Resampling
from skimage.transform import resize, AffineTransform
from skimage.feature import SIFT
from skimage.measure import ransac
import geopandas as gpd
from shapely.geometry import box
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from keras.optimizers import Adam
from sklearn import metrics  
from tensorflow.keras.regularizers import l2
from tensorflow.keras.losses import BinaryCrossentropy
from skimage.feature import graycomatrix, graycoprops

# 讀取多波段影像
def read_multiband_image(image_path):
    with rasterio.open(image_path) as src:
        bands = src.read()  # 讀取所有波段
        image = np.moveaxis(bands, 0, -1)  # 將波段維度移到最後(高、寬、波段)
    return image, src.transform

# 提取B6紅光波段函數
def extract_b6_band(image):
    # 假設B6紅光波段在第6個索引位置 (索引5)
    # 如果波段順序不同，請調整此索引
    if image.shape[2] > 5:
        b6 = image[:, :, 5]
    else:
        # 如果波段數量不足，使用最後一個波段
        b6 = image[:, :, -1]
        print("警告：波段數量不足，使用最後一個波段代替B6紅光波段")
    
    return b6

# 讀取驗證樣本影像
def read_mask(mask_path):
    with rasterio.open(mask_path) as src:
        mask = src.read(1) # 只讀取驗證樣本的第一個波段作為遮罩
    return mask

# 將影像進行正規化
def normalize_image(image):
    # 建立一個與輸入影像相同形狀的空陣列，用來存放正規化後的影像
    normalized_image = np.zeros_like(image, dtype=np.float32)
    
    # 對每個波段分別進行正規化
    for band in range(image.shape[-1]): #影像的波段數量
        band_data = image[..., band] # `...` 表示選取所有高和寬的像素值，`band` 選取當前的波段
        band_min = np.min(band_data)
        band_max = np.max(band_data)
        
        # 判斷最大值是否大於最小值，避免出現全零波段的問題
        if band_max > band_min:
            # 正規化：將每個像素值縮放到 [0, 1] 區間
            normalized_image[..., band] = (band_data - band_min) / (band_max - band_min)
        else:
            # 如果最大值和最小值相等（例如全零波段），保留原始值，避免除以零
            normalized_image[..., band] = band_data # 無法正規化的波段保持不變

    # 返回正規化後的影像        
    return normalized_image

# 計算GLCM特徵
def compute_glcm_features(image, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4]):
    # 確保影像為uint8類型
    if image.dtype != np.uint8:
        # 正規化到0-255範圍並轉換為uint8類型
        img_normalized = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)
    else:
        img_normalized = image
    
    # 計算GLCM
    glcm = graycomatrix(img_normalized, distances=distances, angles=angles, 
                        levels=256, symmetric=True, normed=True)
    
    # 計算GLCM屬性
    contrast = graycoprops(glcm, 'contrast')
    dissimilarity = graycoprops(glcm, 'dissimilarity')
    homogeneity = graycoprops(glcm, 'homogeneity')
    energy = graycoprops(glcm, 'energy')
    correlation = graycoprops(glcm, 'correlation')
    ASM = graycoprops(glcm, 'ASM')  # Angular Second Moment
    
    # 將特徵展平並平均，以獲得單一值表示每個特徵
    contrast_mean = np.mean(contrast)
    dissimilarity_mean = np.mean(dissimilarity)
    homogeneity_mean = np.mean(homogeneity)
    energy_mean = np.mean(energy)
    correlation_mean = np.mean(correlation)
    asm_mean = np.mean(ASM)
    
    # 返回特徵值字典
    return {
        'contrast': contrast_mean,
        'dissimilarity': dissimilarity_mean,
        'homogeneity': homogeneity_mean,
        'energy': energy_mean,
        'correlation': correlation_mean,
        'asm': asm_mean
    }

# 計算災後與災前的GLCM特徵差異
def compute_glcm_change_features(pre_image, post_image, window_size=11, step=3, feature_name="contrast"):
    # 確保是單通道影像
    if len(pre_image.shape) > 2 or len(post_image.shape) > 2:
        raise ValueError("輸入影像必須為單通道灰階影像")
    
    # 確保影像尺寸相同
    if pre_image.shape != post_image.shape:
        raise ValueError("災前與災後影像尺寸必須相同")
    
    # 影像的高度和寬度
    height, width = pre_image.shape
    
    # 初始化特徵變化圖，只包含選定的特徵
    change_features = {
        feature_name: np.zeros((height, width))
    }
    
    # 確保影像為uint8類型
    if pre_image.dtype != np.uint8:
        pre_image_uint8 = ((pre_image - pre_image.min()) / (pre_image.max() - pre_image.min()) * 255).astype(np.uint8)
    else:
        pre_image_uint8 = pre_image
    
    if post_image.dtype != np.uint8:
        post_image_uint8 = ((post_image - post_image.min()) / (post_image.max() - post_image.min()) * 255).astype(np.uint8)
    else:
        post_image_uint8 = post_image
    
    # 計算填充大小
    pad_size = window_size // 2
    
    # 填充影像
    padded_pre_image = np.pad(pre_image_uint8, pad_size, mode='reflect')
    padded_post_image = np.pad(post_image_uint8, pad_size, mode='reflect')
    
    # 計算總處理數量
    total_pixels = ((height - 1) // step + 1) * ((width - 1) // step + 1)
    processed_pixels = 0
    
    print(f"開始計算 {feature_name} GLCM特徵變化，共需處理 {total_pixels} 個像素位置...")
    
    # 對每個像素位置計算GLCM特徵變化
    for i in range(0, height, step):
        for j in range(0, width, step):
            # 提取窗口
            pre_window = padded_pre_image[i:i+window_size, j:j+window_size]
            post_window = padded_post_image[i:i+window_size, j:j+window_size]
            
            # 計算災前與災後的GLCM特徵，只計算選定的特徵
            pre_features = compute_glcm_features(pre_window)
            post_features = compute_glcm_features(post_window)
            
            # 計算特徵差異
            change_value = abs(post_features[feature_name] - pre_features[feature_name])
            # 將變化值填入特徵圖對應位置
            change_features[feature_name][i, j] = change_value
            
            # 更新處理進度
            processed_pixels += 1
            
            # 每處理100個像素或處理完最後一個像素時更新進度
            if processed_pixels % 100 == 0 or processed_pixels == total_pixels:
                progress = (processed_pixels / total_pixels) * 100
                print(f"\r處理進度: [{processed_pixels}/{total_pixels}] {progress:.2f}%", end="")
    
    print(f"\n{feature_name} GLCM特徵變化計算完成！")
    
    # 對於沒有直接計算的像素，使用插值填充
    if step > 1:
        # 創建完整尺寸的索引矩陣
        full_indices = np.mgrid[0:height, 0:width]
        # 創建已計算點的索引矩陣
        y_indices, x_indices = np.mgrid[0:height:step, 0:width:step]
        # 取得已計算點的值
        values = change_features[feature_name][0:height:step, 0:width:step]
        # 使用線性插值填充其他點
        from scipy.interpolate import griddata
        points = np.vstack([y_indices.ravel(), x_indices.ravel()]).T
        values_flat = values.ravel()
        grid_y, grid_x = full_indices
        change_features[feature_name] = griddata(points, values_flat, (grid_y, grid_x), method='linear', fill_value=0)
    
    return change_features

# 提取影像特徵點
def extract_features(image):
    # 檢查影像是否為空值
    if image is None:
        print("影像無法讀取或為空值")
        return None, None
    
    # 確保影像是 uint8 類型
    if image.dtype != np.uint8:
        image = image.astype(np.uint8)
    
    # 檢查影像的通道數
    if len(image.shape) == 3:
        # 若為多波段影像選擇紅光波段
        if image.shape[2] == 8:  # 假設影像是 8 波段
            gray = image[:, :, 0]  # 使用紅光波段(第0波段)
        else:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # 彩色影像轉為灰階影像
    else:
        gray = image  # 灰階影像直接返回

    # 使用 SIFT 檢測特徵點(邊緣特徵)
    sift = cv2.SIFT_create()
    # 自動檢測灰階單波段影像中的所有特徵點
    keypoints, descriptors = sift.detectAndCompute(gray, None)
    
    return keypoints, descriptors

# 匹配特徵點
def match_features(desc1, desc2):

    # 使用 FLANN 匹配器
    FLANN_INDEX_KDTREE = 1
    # 使用 KD-Tree 作為最近鄰搜索算法
    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
    # 數值越大準確度越高
    search_params = dict(checks=50)
    # 進行 KNN（k 最近鄰）匹配
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    # 兩張影像要匹配的最近鄰數量
    matches = flann.knnMatch(desc1, desc2, k=2)
    
    # 過濾匹配錯誤
    good_matches = []
    # 最近鄰（m）與次近鄰（n）之間的距離比值小於0.7，可靠性越高
    for m, n in matches:
        if m.distance < 0.7 * n.distance:
            good_matches.append(m)
            
    return good_matches

# 對齊影像，將一張影像對齊到參考影像
def align_image(img_to_align, ref_img):
    # 提取特徵點
    kp1, des1 = extract_features(ref_img)
    kp2, des2 = extract_features(img_to_align)
    
    if des1 is None or des2 is None:
        print("無法檢測到足夠的特徵點，使用簡單的大小調整")
        return cv2.resize(img_to_align, (ref_img.shape[1], ref_img.shape[0]))
    
    # 匹配特徵點
    matches = match_features(des1, des2)
    
    if len(matches) < 4:
        print("匹配點太少，使用簡單的大小調整")
        return cv2.resize(img_to_align, (ref_img.shape[1], ref_img.shape[0]))
    
    # 獲取匹配點的座標
    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 2)
    
    # 使用 RANSAC 估計變換矩陣
    transform_matrix, inliers = cv2.estimateAffinePartial2D(src_pts, dst_pts)
    
    if transform_matrix is None:
        print("無法估計變換矩陣，使用簡單的大小調整")
        return cv2.resize(img_to_align, (ref_img.shape[1], ref_img.shape[0]))
    
    # 對影像進行仿射變換
    # 待對齊影像應用一個仿射變換矩陣，使其對齊到參考影像的空間範圍和大小
    aligned_img = cv2.warpAffine(img_to_align, transform_matrix, 
                                 (ref_img.shape[1], ref_img.shape[0]))
    
    return aligned_img

# 對齊Google與GSI影像，使其與原始影像一致
def align_png_images(original_image, google_png, gsi_png, patch_size=64):

    # 確保參考影像大小合適
    target_height = (original_image.shape[0] // patch_size) * patch_size
    target_width = (original_image.shape[1] // patch_size) * patch_size
    
    # 調整參考影像大小
    ref_image = cv2.resize(original_image, (target_width, target_height))
    
    # 對齊 Google Maps 影像
    print("正在對齊 Google Maps 影像...")
    aligned_google = align_image(google_png, ref_image)
    
    # 對齊 GSI 影像
    print("正在對齊 GSI 影像...")
    aligned_gsi = align_image(gsi_png, ref_image)
    
    # 後處理：確保所有影像具有相同的大小和格式
    def post_process(img):
        # 確保大小一致
        img = cv2.resize(img, (target_width, target_height))
        # 正規化到 0-1 範圍
        img = img.astype(np.float32) / 255.0
        return img
    # 對齊後處理
    aligned_google = post_process(aligned_google)
    aligned_gsi = post_process(aligned_gsi)
    
    return ref_image, aligned_google, aligned_gsi

# 提取B6紅光波段並計算GLCM特徵
def extract_b6_and_compute_glcm(image):
    print("開始提取B6紅光波段並計算GLCM特徵...")
    
    # 提取B6紅光波段 (假設是波段索引5)
    print("提取B6紅光波段...")
    b6_band = image[:, :, 5]
    
    # 正規化B6波段到0-1範圍
    print("正規化B6波段...")
    b6_normalized = (b6_band - b6_band.min()) / (b6_band.max() - b6_band.min())
    
    # 計算GLCM特徵圖
    print("計算GLCM特徵圖...")
    glcm_features = compute_glcm_change_features(b6_normalized, window_size=11, step=1)
    
    # 將GLCM特徵和原始波段合併為一個多通道影像
    print("合併GLCM特徵和原始波段...")
    # 初始化輸出影像
    output_image = np.zeros((image.shape[0], image.shape[1], image.shape[2] + len(glcm_features)))
    
    # 複製原始波段
    output_image[:, :, :image.shape[2]] = image
    
    # 添加GLCM特徵
    feature_idx = image.shape[2]
    for feature_name, feature_map in glcm_features.items():
        output_image[:, :, feature_idx] = feature_map
        feature_idx += 1
        print(f"已添加 {feature_name} 特徵")
    
    print("B6紅光波段GLCM特徵計算完成！")
    return output_image, glcm_features

# # 繪製GLCM特徵變化直方圖
# def plot_glcm_change_histograms(change_features, mask, output_folder, pair_name):

#     # 確保輸出資料夾存在
#     if not os.path.exists(output_folder):
#         os.makedirs(output_folder)
    
#     # 二值化標籤遮罩
#     binary_mask = (mask > 0).astype(bool)
    
#     # 計算總特徵數量
#     total_features = len(change_features)
    
#     print(f"開始繪製 {pair_name} 的GLCM特徵變化直方圖，共 {total_features} 個特徵...")
    
#     # 對每個特徵繪製直方圖
#     for idx, (feature_name, feature_map) in enumerate(change_features.items(), 1):
#         # 顯示進度
#         print(f"\r繪製進度: [{idx}/{total_features}] {feature_name} 特徵變化", end="")
        
#         # 獲取變化區域和無變化區域的特徵變化值
#         change_values = feature_map[binary_mask]
#         no_change_values = feature_map[~binary_mask]
        
#         # 創建圖表
#         plt.figure(figsize=(12, 6))
        
#         # 設置直方圖參數
#         bins = 30
#         alpha = 0.7
        
#         # 繪製無變化區域的直方圖
#         plt.hist(no_change_values, bins=bins, alpha=alpha, label='No Change Area', color='blue')
        
#         # 繪製變化區域的直方圖
#         plt.hist(change_values, bins=bins, alpha=alpha, label='Change Area', color='red')
        
#         # 添加標題和標籤
#         plt.title(f'GLCM {feature_name} Feature Difference Histogram')
#         plt.xlabel(f'{feature_name} Change Value')
#         plt.ylabel('Frequency')
#         plt.legend()
#         plt.grid(True, alpha=0.3)
        
#         # 保存圖表
#         plt.savefig(os.path.join(output_folder, f'{pair_name}_{feature_name}_change_histogram.png'))
#         plt.close()
    
#     print("\nGLCM特徵變化直方圖繪製完成！")
def process_reference_images(google_path, gsi_path, target_height, target_width, patch_size=64):

    # 讀取參考影像
    google_img = cv2.imread(google_path)
    gsi_img = cv2.imread(gsi_path)
    
    print("原始 Google Map 影像尺寸:", google_img.shape)
    print("原始 GSI 影像尺寸:", gsi_img.shape)
    
    # 使用 resize縮放到指定大小
    full_google = cv2.resize(google_img, (target_width, target_height), 
                              interpolation=cv2.INTER_AREA)
    full_gsi = cv2.resize(gsi_img, (target_width, target_height), 
                           interpolation=cv2.INTER_AREA)
    
    print("縮放後 Google Map 影像尺寸:", full_google.shape)
    print("縮放後 GSI 影像尺寸:", full_gsi.shape)
    
    # 正規化影像
    google_normalized = full_google.astype(float) / 255.0
    gsi_normalized = full_gsi.astype(float) / 255.0
    
    # 分割影像
    google_patches = patchify(google_normalized, (patch_size, patch_size, 3), step=patch_size)
    gsi_patches = patchify(gsi_normalized, (patch_size, patch_size, 3), step=patch_size)
    
    # 計算正確的切割順序
    rows = google_patches.shape[0]
    cols = google_patches.shape[1]
    total_patches = rows * cols

    print(f"影像切割資訊：")
    print(f"總行數：{rows}")
    print(f"總列數：{cols}")
    print(f"總切割數：{total_patches}")
    
    google_patches_reordered = []
    gsi_patches_reordered = []
    
    for i in range(rows):
        for j in range(cols-1, -1, -1):  
            google_patches_reordered.append(google_patches[i, j, 0])
            gsi_patches_reordered.append(gsi_patches[i, j, 0])
    
    # 轉換為 numpy 陣列
    google_patches_reordered = np.array(google_patches_reordered)
    gsi_patches_reordered = np.array(gsi_patches_reordered)
    
    return google_patches_reordered, gsi_patches_reordered, full_google, full_gsi


def preprocess_data(X, y):

    # 對輸入資料進行標準化
    X_processed = X.astype('float32')
    
    # 確保 y 是三維的，形狀為 (n_samples, height, width, 1)
    y_processed = y.astype('float32')
    # 如果 y 是二維的，則添加一個通道維度
    if len(y_processed.shape) == 3:
        y_processed = np.expand_dims(y_processed, axis=-1)
    
    return X_processed, y_processed

# def split_data_by_pairs(all_pair_data, train_ratio=0.85, val_from_train_ratio=0.15):
#     # 檢查是否有足夠的數據
#     if len(all_pair_data) == 0:
#         print("警告：沒有輸入數據！")
#         return np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])
    
#     # 確保至少有一對用於訓練
#     if train_ratio <= 0 or train_ratio >= 1:
#         print("警告：train_ratio 應該在 (0, 1) 範圍內，設置為預設值 0.85")
#         train_ratio = 0.85
        
#     # 打亂影像對的順序
#     np.random.shuffle(all_pair_data)
    
#     # 確保至少有一對用於訓練和一對用於測試
#     n_pairs = len(all_pair_data)
#     train_val_split = max(1, int(n_pairs * train_ratio))
    
#     # 分割為訓練+驗證集 和 測試集
#     train_val_pairs = all_pair_data[:train_val_split]
#     test_pairs = all_pair_data[train_val_split:]
    
#     # 確保訓練集至少有一對
#     n_train_val = len(train_val_pairs)
#     train_split = max(1, int(n_train_val * (1 - val_from_train_ratio)))
    
#     train_pairs = train_val_pairs[:train_split]
#     val_pairs = train_val_pairs[train_split:]
    
#     # 合併每個集合中的所有patches
#     def combine_pairs(pairs):
#         if not pairs:  # 如果為空列表，返回正確形狀的空陣列
#             # 獲取原始數據的形狀
#             sample_shape = all_pair_data[0]['X'].shape[1:]  # 排除批次維度
#             return np.zeros((0, *sample_shape)), np.zeros((0, *all_pair_data[0]['y'].shape[1:]))
#         X = np.concatenate([p['X'] for p in pairs])
#         y = np.concatenate([p['y'] for p in pairs])
#         return X, y
    
#     X_train, y_train = combine_pairs(train_pairs)
#     X_val, y_val = combine_pairs(val_pairs)
#     X_test, y_test = combine_pairs(test_pairs)
    
#     # 輸出分割資訊
#     print(f"資料分割: 訓練集 {len(train_pairs)}/{n_pairs} 對影像 ({len(train_pairs)/n_pairs*100:.1f}%)")
#     print(f"          驗證集 {len(val_pairs)}/{n_pairs} 對影像 ({len(val_pairs)/n_pairs*100:.1f}%)")
#     print(f"          測試集 {len(test_pairs)}/{n_pairs} 對影像 ({len(test_pairs)/n_pairs*100:.1f}%)")
    
#     return X_train, X_val, X_test, y_train, y_val, y_test

# 定義U-Net模型
def unet_model(input_size=(64, 64, 3), learning_rate=0.002):  # 1個原始波段 + 1個GLCM特徵+1個綜合特徵
    inputs = Input(input_size)
    # 設定丟棄率
    dropoutRate = 0.3
    
    # 編碼器路徑
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = Dropout(dropoutRate)(conv1)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = Dropout(dropoutRate)(conv2)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = Dropout(dropoutRate)(conv3)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = Dropout(dropoutRate)(conv4)
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)
    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)
    
    # 橋接
    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(pool4)
    conv5 = Conv2D(1024, 3, activation='relu', padding='same')(conv5)
    conv5 = Dropout(dropoutRate)(conv5)
    
    # 解碼器路徑
    up6 = Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(conv5)
    merge6 =Concatenate(axis=3)([conv4, up6])
    conv6 = Conv2D(512, 3, activation='relu', padding='same')(merge6)
    conv6 = Conv2D(512, 3, activation='relu', padding='same')(conv6)
    conv6 = Dropout(dropoutRate)(conv6)
    
    up7 = Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)
    merge7 = Concatenate(axis=3)([conv3, up7])  
    conv7 = Conv2D(256, 3, activation='relu', padding='same')(merge7)
    conv7 = Conv2D(256, 3, activation='relu', padding='same')(conv7)
    conv7 = Dropout(dropoutRate)(conv7)

    up8 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)
    merge8 = Concatenate(axis=3)([conv2, up8])  
    conv8 = Conv2D(128, 3, activation='relu', padding='same')(merge8)
    conv8 = Conv2D(128, 3, activation='relu', padding='same')(conv8)
    conv8 = Dropout(dropoutRate)(conv8)

    up9 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)
    merge9 = Concatenate(axis=3)([conv1, up9])  
    conv9 = Conv2D(64, 3, activation='relu', padding='same')(merge9)
    conv9 = Conv2D(64, 3, activation='relu', padding='same')(conv9)
    conv9 = Dropout(dropoutRate)(conv9)
    
    # 輸出層
    outputs = Conv2D(1, 1, activation='sigmoid')(conv9)
    
    model = Model(inputs=inputs, outputs=outputs)
    def weighted_binary_crossentropy(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        
        # 使用較小的正類別權重，避免過多假陽性
        pos_weight =7
        
        # 計算加權損失
        loss_pos = y_true * tf.math.log(tf.clip_by_value(y_pred, 1e-7, 1.0)) * pos_weight
        loss_neg = (1 - y_true) * tf.math.log(tf.clip_by_value(1 - y_pred, 1e-7, 1.0))
        
        return -tf.reduce_mean(loss_pos + loss_neg)

    # 添加Dice損失以改善分割效果
    def dice_loss(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        numerator = 2 * tf.reduce_sum(y_true * y_pred)
        denominator = tf.reduce_sum(tf.square(y_true) + tf.square(y_pred))
        
        return 1 - numerator / (denominator + 1e-7)

    # 組合損失函數
    def combined_loss(y_true, y_pred):
        # 50% BCE + 50% Dice - 平衡分類和分割效果
        return 0.5 * weighted_binary_crossentropy(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)

    # 更新模型編譯
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),  # 降低學習率
        loss=combined_loss,
        metrics=['accuracy']
    )
    
    return model

# 計算最佳閾值和評估指標
def calculate_optimal_threshold(change_features, mask):
    # 二值化標籤遮罩
    binary_mask = (mask > 0).astype(bool)
    
    # 初始化結果字典
    results = {}
    
    print("計算各特徵的最佳閾值和評估指標...")
    
    # 對每個特徵計算最佳閾值和評估指標
    for feature_name, feature_map in change_features.items():
        # 獲取變化區域和無變化區域的特徵變化值
        change_values = feature_map[binary_mask]
        no_change_values = feature_map[~binary_mask]
        
        # 計算特徵值的最小和最大值
        min_value = min(np.min(change_values), np.min(no_change_values))
        max_value = max(np.max(change_values), np.max(no_change_values))
        
        # 生成閾值列表
        thresholds = np.linspace(min_value, max_value, 100)
        
        # 初始化評估指標
        best_threshold = 0
        best_f1 = 0
        best_accuracy = 0
        best_precision = 0
        best_recall = 0
        
        # 對每個閾值計算評估指標
        for threshold in thresholds:
            # 根據閾值生成二值化預測
            prediction = (feature_map > threshold).astype(bool)
            
            # 計算評估指標
            TP = np.sum(prediction & binary_mask)
            FP = np.sum(prediction & ~binary_mask)
            TN = np.sum(~prediction & ~binary_mask)
            FN = np.sum(~prediction & binary_mask)
            
            # 計算精確度、召回率、F1分數和準確率
            precision = TP / (TP + FP) if (TP + FP) > 0 else 0
            recall = TP / (TP + FN) if (TP + FN) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            accuracy = (TP + TN) / (TP + TN + FP + FN)
            
            # 更新最佳結果
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
                best_accuracy = accuracy
                best_precision = precision
                best_recall = recall
        
        # 將結果添加到結果字典
        results[feature_name] = {
            'threshold': best_threshold,
            'accuracy': best_accuracy,
            'precision': best_precision,
            'recall': best_recall,
            'f1': best_f1
        }
    
    return results

def visualize_results(original_image, normalized_post, gsi_image, label_image, pred_image, output_folder):

    # 設定輸出檔案名稱
    output_file_name = "visualization_result V2.png"
    output_path = os.path.join(output_folder, output_file_name)

    red_channel = original_image[..., 5]  # Band 6: 紅光
    green_channel = original_image[..., 3]  # Band 4: 綠光
    blue_channel = original_image[..., 1]  # Band 2: 藍光

    # 將RGB三個波段組合成彩色影像
    rgb_image = np.stack([red_channel, green_channel, blue_channel], axis=-1)
    # 正規化 RGB 通道到 [0, 1] 範圍，避免過亮或過暗
    rgb_normalized = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min())
        
    # 應用 gamma 校正
    gamma = 0.5 # gamma值小於1會增加影像亮度
    rgb_corrected = np.power(rgb_normalized, gamma)

    red_channel = normalized_post[..., 5]  # Band 6: 紅光
    green_channel = normalized_post[..., 3]  # Band 4: 綠光
    blue_channel = normalized_post[..., 1]  # Band 2: 藍光

    # 將RGB三個波段組合成彩色影像
    rgb_image_post = np.stack([red_channel, green_channel, blue_channel], axis=-1)
    # 正規化 RGB 通道到 [0, 1] 範圍，避免過亮或過暗
    rgb_normalized_post = (rgb_image_post - rgb_image_post.min()) / (rgb_image_post.max() - rgb_image_post.min())
        
    # 應用 gamma 校正
    gamma = 0.5 # gamma值小於1會增加影像亮度
    rgb_corrected_post = np.power(rgb_normalized_post, gamma)

    # 將預測影像轉換為原始大小
    pred_patches = pred_image.reshape(7, 8, 64, 64)  
    full_pred = unpatchify(pred_patches, (448, 512))  
    full_pred = (full_pred > 0.5).astype(np.uint8)  # 二值化預測結果

    # 設置圖形大小
    plt.figure(figsize=(15, 15))

    # 顯示原始影像（災前影像）
    plt.subplot(2, 3, 1)
    plt.imshow(rgb_corrected)
    plt.title("Original Image (Pre-disaster)")
    plt.axis('off')

    plt.subplot(2, 3, 2)
    plt.imshow(rgb_corrected_post)
    plt.title("Original Image (Post-disaster)")
    plt.axis('off')

    plt.subplot(2,3, 3)
    plt.imshow(gsi_image)
    plt.title("GSI Image")
    plt.axis('off')

    # 顯示真實標籤
    plt.subplot(2, 3, 4)
    plt.imshow(label_image, cmap='gray')
    plt.title("Ground Truth (Change Label)")
    plt.axis('off')
    
    # 顯示二值化預測結果
    plt.subplot(2, 3, 5)
    plt.imshow(full_pred, cmap='gray')
    plt.title("Prediction (Change Detection)")
    plt.axis('off')

    comparison = np.zeros((*label_image.shape, 3))
    comparison[..., 0] = (full_pred == 1) & (label_image== 0)  # FP
    comparison[..., 1] = (full_pred == 1) & (label_image== 1)  # TP
    comparison[..., 2] = (full_pred == 0) & (label_image == 1)  # FN
    
    plt.subplot(2, 3, 6)
    plt.imshow(comparison)
    plt.title(f"Comparison Patches \nGreen: TP, Red: FP, Blue: FN")
    plt.axis('off')

    # 儲存視覺化結果
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()

    print(f"視覺化結果已儲存到: {output_path}")

def visualize_multispectral_image(original_image, rgb_corrected_post, gsi_image, label_image, pred_image, output_folder=None):
    def process_image(img, is_multispectral=True):
        if is_multispectral:
            if img.shape[-1] >= 6:
                rgb = np.stack([img[..., 5], img[..., 3], img[..., 1]], axis=-1)
            else:
                rgb = img
            rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())
            p_low, p_high = np.percentile(rgb_norm, (2, 98))
            rgb_stretched = np.clip((rgb_norm - p_low) / (p_high - p_low), 0, 1)
            gamma = 0.6
            rgb_gamma = np.power(rgb_stretched, gamma)
            return rgb_gamma
        else:
            return img

    rows = original_image.shape[0] // 64
    cols = original_image.shape[1] // 64

    for row in range(rows):
        for col in range(cols):
            start_h = row * 64
            start_w = col * 64
            end_h = start_h + 64
            end_w = start_w + 64

            current_patches = {
                'original': process_image(original_image[start_h:end_h, start_w:end_w], True),
                'original_post': process_image(rgb_corrected_post[start_h:end_h, start_w:end_w], True),
                'gsi': process_image(gsi_image[start_h:end_h, start_w:end_w], False),
                'label': label_image[start_h:end_h, start_w:end_w],
                'pred': pred_image[start_h:end_h, start_w:end_w]
            }

            plt.figure(figsize=(15, 10))
            
            plt.subplot(2, 3, 1)
            plt.imshow(current_patches['original'])
            plt.title(f"Original Patch ({row},{col})")
            plt.axis('off')
            
            plt.subplot(2, 3, 2)
            plt.imshow(current_patches['original_post'])
            plt.title(f"Original Post Patch ({row},{col})")
            plt.axis('off')
            
            plt.subplot(2, 3, 3)
            plt.imshow(current_patches['gsi'])
            plt.title(f"GSI Patch ({row},{col})")
            plt.axis('off')
            
            plt.subplot(2, 3, 4)
            plt.imshow(current_patches['label'], cmap='magma')
            plt.title(f"Validations ({row},{col})")
            plt.axis('off')
            
            plt.subplot(2, 3, 5)
            plt.imshow(current_patches['pred'], cmap='magma')
            plt.title(f"Predictions Patch ({row},{col})")
            plt.axis('off')
            
            comparison = np.zeros((*current_patches['pred'].shape, 3))
            comparison[..., 0] = (current_patches['pred'] == 1) & (current_patches['label'] == 0)  # FP
            comparison[..., 1] = (current_patches['pred'] == 1) & (current_patches['label'] == 1)  # TP
            comparison[..., 2] = (current_patches['pred'] == 0) & (current_patches['label'] == 1)  # FN
            
            plt.subplot(2, 3, 6)
            plt.imshow(comparison)
            plt.title(f"Comparison Patch ({row},{col})\nGreen: TP, Red: FP, Blue: FN")
            plt.axis('off')
            
            plt.tight_layout()
            
            if output_folder:
                filename = f"patch_combine_V2_{row}_{col}.png"
                plt.savefig(os.path.join(output_folder, filename), dpi=300, bbox_inches='tight')
            plt.close()

    # 計算切割網格範圍
    patch_size = 64
    height, width = original_image.shape[:2]

    # 建立視覺化影像
    plt.figure(figsize=(10, 10))
    plt.imshow(process_image(original_image))
    
    # 繪製 64x64 格線
    for y in range(0, height, patch_size):  # 水平線
        plt.axhline(y=y, color='r', linestyle='--', alpha=0.5)
    for x in range(0, width, patch_size):  # 垂直線
        plt.axvline(x=x, color='r', linestyle='--', alpha=0.5)

    plt.title("Patch Overview")
    if output_folder:
        plt.savefig(os.path.join(output_folder, "patch_overview.png"), dpi=300, bbox_inches='tight')
    plt.close()

def calculate_optimal_threshold_single(feature_map, mask):
    # 二值化標籤遮罩
    binary_mask = (mask > 0).astype(bool)
    
    # 獲取變化區域和無變化區域的特徵變化值
    change_values = feature_map[binary_mask]
    no_change_values = feature_map[~binary_mask]
    
    # 如果資料為空，返回預設值
    if len(change_values) == 0 or len(no_change_values) == 0:
        return {'threshold': 0.5, 'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}
    
    # 創建可能的閾值列表
    all_values = np.concatenate([change_values, no_change_values])
    thresholds = np.linspace(all_values.min(), all_values.max(), 100)
    
    # 初始化最佳指標
    best_metrics = {'threshold': 0, 'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}
    
    # 對每個閾值進行評估
    for threshold in thresholds:
        # 根據閾值創建預測結果
        prediction = (feature_map > threshold).astype(bool)
        
        # 計算混淆矩陣
        TP = np.sum(prediction & binary_mask)
        FP = np.sum(prediction & ~binary_mask)
        TN = np.sum(~prediction & ~binary_mask)
        FN = np.sum(~prediction & binary_mask)
        
        # 計算評估指標
        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # 更新最佳指標
        if f1 > best_metrics['f1']:
            best_metrics = {
                'threshold': threshold,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
    
    return best_metrics

def main():
    patchsize = 64  # 切割影像大小
    step = patchsize   # 切割影像的步長，與 patchsize 相同表示無重疊
    pre_collapse_folder = r"E:\NTOU\meeting\Wajima_pre_test_230401" # 災前影像資料夾
    post_collapse_folder = r"E:\NTOU\meeting\Wajima_post_test_240410" # 災後影像資料夾
    validation_folder = r"E:\NTOU\meeting\Wajima_validation_images" # 驗證樣本資料夾
    output_folder = r"E:\NTOU\meeting\CNN\Wajima_visualization_results2" # 結果輸出資料夾
    glcm_output_folder = os.path.join(output_folder, "glcm_histograms")  # GLCM直方圖輸出資料夾
    google_map_path = r"E:\NTOU\meeting\輪島Google.png" # Google Map 影像路徑 (地震前)
    gsi_map_path = r"E:\NTOU\meeting\輪島國土.png"  # GSI 影像路徑 (地震後)
    
    # 確保輸出資料夾存在，若不存在則建立
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    if not os.path.exists(glcm_output_folder):
        os.makedirs(glcm_output_folder)

    # 定義災前和災後影像的對應關係（按時間順序）
    image_pairs = [("20230401_pre.tif", "20240410_post.tif"),]

    # 獲取所有驗證用的標籤影像檔案名稱
    label_files = [f for f in os.listdir(validation_folder) if f.endswith(".tif") and "label" in f]
    print(f"Label files: {label_files}")

    # 讀取參考影像 (Google Map 和 GSI)
    google_img = cv2.imread(google_map_path)
    gsi_img = cv2.imread(gsi_map_path)
    
    # 初始化用於存放影像與標籤的列表
    X_all = [] # 儲存災前災後影像差異（包含GLCM特徵）
    y_all = []  # 儲存標籤 (有無變化)
    original_patches_all = [] # 儲存原始災前影像的切割影像
    patch_locations = []  # 用來記錄每個patch的位置

    # 使用定義好的影像對來進行處理
    for idx, (pre_img, post_img) in enumerate(image_pairs):
        print(f"處理第 {idx+1} 對影像: {pre_img} - {post_img}")
        
        img_path_pre = os.path.join(pre_collapse_folder, pre_img)
        img_path_post = os.path.join(post_collapse_folder, post_img)
        
        # 對每個標籤檔案進行處理
        for label_file in label_files:
            label_path = os.path.join(validation_folder, label_file)
            print(f"使用標籤檔案: {label_file}")

            # 讀取影像
            image_pre, _ = read_multiband_image(img_path_pre)
            image_post, _ = read_multiband_image(img_path_post)
            mask = read_mask(label_path)    # 讀取驗證樣本遮罩

            # 計算需要的填充大小
            target_height = (image_pre.shape[0] // 64) * 64  # 448
            target_width = (image_pre.shape[1] // 64) * 64    # 512
            
            # 裁剪影像
            image_pre = image_pre[:target_height, :target_width, :]
            image_post = image_post[:target_height, :target_width, :]
            mask = mask[:target_height, :target_width]

            # 將Google和GSI影像對齊
            ref_image, aligned_google, aligned_gsi = align_png_images(image_pre, google_img, gsi_img, patchsize)

            # 對災前災後影像進行正規化處理（將像素值縮放到0-1範圍）
            print("正規化前災前影像像素範圍:", image_pre.min(), image_pre.max())
            print("正規化前災後影像像素範圍:", image_post.min(), image_post.max())
            
            normalized_pre = normalize_image(image_pre)
            normalized_post = normalize_image(image_post)
            
            print("正規化後災前影像像素範圍:", normalized_pre.min(), normalized_pre.max())
            print("正規化後災後影像像素範圍:", normalized_post.min(), normalized_post.max())
            
            # 提取B6紅光波段
            b6_pre = extract_b6_band(normalized_pre)
            b6_post = extract_b6_band(normalized_post)
            
            selected_feature = "contrast"  # 可選: "contrast", "dissimilarity", "homogeneity", "energy", "correlation", "asm"
            
            # 在主要處理流程中,告知使用者選擇的特徵
            print(f"使用單一 GLCM 特徵: {selected_feature}")

            # 直接計算災後-災前的GLCM特徵變化
            print("直接計算災後-災前的GLCM特徵變化...")
            glcm_change_features = compute_glcm_change_features(b6_pre, b6_post, window_size=11, step=3, feature_name=selected_feature)
            
            # 繪製GLCM特徵變化直方圖
            print("繪製GLCM特徵變化直方圖...")
            # plot_glcm_change_histograms(glcm_change_features, mask, glcm_output_folder, f"{pre_img.split('.')[0]}_{post_img.split('.')[0]}")
            
            # 計算每個特徵的最佳閾值和評估指標
            # 將這部分:
            # 計算每個特徵的最佳閾值和評估指標
            print("計算最佳閾值和評估指標...")
            threshold_results = calculate_optimal_threshold(glcm_change_features, mask)

            # 顯示結果
            print("\n各特徵的最佳閾值和評估指標:")
            for feature_name, metrics in threshold_results.items():
                print(f"{feature_name}:")
                print(f"  最佳閾值: {metrics['threshold']:.4f}")
                print(f"  準確率: {metrics['accuracy']:.4f}")
                print(f"  精確度: {metrics['precision']:.4f}")
                print(f"  召回率: {metrics['recall']:.4f}")
                print(f"  F1分數: {metrics['f1']:.4f}")

            # 根據計算結果，找出表現最好的特徵
            best_feature = max(threshold_results.items(), key=lambda x: x[1]['f1'])
            print(f"\n表現最佳的特徵是 {best_feature[0]}，F1分數為 {best_feature[1]['f1']:.4f}")

            # 修改為:
            # 計算選定特徵的最佳閾值和評估指標
            print(f"計算 {selected_feature} 特徵的最佳閾值和評估指標...")
            threshold_result = calculate_optimal_threshold_single(glcm_change_features[selected_feature], mask)

            # 顯示結果
            print(f"\n{selected_feature} 特徵的最佳閾值和評估指標:")
            print(f"  最佳閾值: {threshold_result['threshold']:.4f}")
            print(f"  準確率: {threshold_result['accuracy']:.4f}")
            print(f"  精確度: {threshold_result['precision']:.4f}")
            print(f"  召回率: {threshold_result['recall']:.4f}")
            print(f"  F1分數: {threshold_result['f1']:.4f}")
            # 根據計算結果，找出表現最好的特徵
            # best_feature = max(threshold_results.items(), key=lambda x: x[1]['f1'])

            # 使用最佳特徵生成變化預測圖
            best_threshold = threshold_result['threshold']
            change_prediction = (glcm_change_features[selected_feature] > best_threshold).astype(np.uint8) * 255
            
            # 保存預測結果
            cv2.imwrite(os.path.join(output_folder, f"{pre_img.split('.')[0]}_{post_img.split('.')[0]}_prediction.png"), change_prediction)
            
            # 創建二值化的變化檢測結果
            binary_mask = (mask > 0).astype(np.uint8) * 255
            
            # 保存真實標籤
            cv2.imwrite(os.path.join(output_folder, f"{label_file.split('.')[0]}_binary.png"), binary_mask)
            
            # 計算混淆矩陣
            TP = np.sum((change_prediction == 255) & (binary_mask == 255))
            FP = np.sum((change_prediction == 255) & (binary_mask == 0))
            TN = np.sum((change_prediction == 0) & (binary_mask == 0))
            FN = np.sum((change_prediction == 0) & (binary_mask == 255))
            
            # 計算指標
            accuracy = (TP + TN) / (TP + TN + FP + FN)
            precision = TP / (TP + FP) if (TP + FP) > 0 else 0
            recall = TP / (TP + FN) if (TP + FN) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # 根據計算結果，找出表現最好的特徵
            best_feature = max(threshold_results.items(), key=lambda x: x[1]['f1'])
            print(f"\n表現最佳的特徵是 {best_feature[0]}，F1分數為 {best_feature[1]['f1']:.4f}")
            
            # 使用最佳特徵生成變化預測圖
            best_feature_name = best_feature[0]
            print(f"\n最佳特徵 {best_feature_name} 的變化檢測結果:")
            print(f"  真陽性(TP): {TP}")
            print(f"  假陽性(FP): {FP}")
            print(f"  真陰性(TN): {TN}")
            print(f"  假陰性(FN): {FN}")
            print(f"  準確率: {accuracy:.4f}")
            print(f"  精確度: {precision:.4f}")
            print(f"  召回率: {recall:.4f}")
            print(f"  F1分數: {f1:.4f}")

            # 將正規化後的影像切割成小塊（patches）
            # 使用差異最大的GLCM特徵作為增強特徵
            enhanced_diff = glcm_change_features[best_feature_name]
            
            enhanced_pre = np.zeros((b6_pre.shape[0], b6_pre.shape[1], 3))
            enhanced_post = np.zeros((b6_post.shape[0], b6_post.shape[1], 3))

            # 第一通道為原始B6波段
            enhanced_pre[:,:,0] = b6_pre
            enhanced_post[:,:,0] = b6_post

            # 第二通道為選定的GLCM特徵
            enhanced_pre[:,:,1] = glcm_change_features[selected_feature]
            enhanced_post[:,:,1] = glcm_change_features[selected_feature]

            # 第三通道為原始B6波段與選定GLCM特徵的組合
            enhanced_pre[:,:,2] = (b6_pre + glcm_change_features[selected_feature]) / 2
            enhanced_post[:,:,2] = (b6_post + glcm_change_features[selected_feature]) / 2
                        
            # 正規化增強影像
            enhanced_pre = normalize_image(enhanced_pre)
            enhanced_post = normalize_image(enhanced_post)
            
            # 計算差異影像
            enhanced_diff_img = enhanced_post - enhanced_pre
            
            
            # 切割影像
            patches_pre = patchify(enhanced_pre, (patchsize, patchsize, enhanced_pre.shape[-1]), step=step)
            patches_post = patchify(enhanced_post, (patchsize, patchsize, enhanced_post.shape[-1]), step=step)
            patches_diff = patchify(enhanced_diff_img, (patchsize, patchsize, enhanced_diff_img.shape[-1]), step=step)
            patches_mask = patchify(mask, (patchsize, patchsize), step=step)

            # 重塑 patches 以便處理
            n_patches_h = patches_pre.shape[0]
            n_patches_w = patches_pre.shape[1]
            
            for i in range(n_patches_h):
                for j in range(n_patches_w):
                    # 取得當前 patch
                    patch_pre = patches_pre[i, j, 0]
                    patch_post = patches_post[i, j, 0]
                    patch_diff = patches_diff[i, j, 0]
                    patch_mask = patches_mask[i, j]
                    
                    # 儲存資料
                    X_all.append(patch_diff.astype(np.float32))
                    y_all.append(patch_mask.astype(np.uint8))
                    original_patches_all.append(patch_pre)  # 儲存原始災前影像的切割影像
                    patch_locations.append((i, j))  # 記錄位置信息
                    
            print(f"該對影像產生了 {n_patches_h * n_patches_w} 個 patches")
            
    # 將資料轉換為 numpy 陣列
    X_all = np.array(X_all)
    y_all = np.array(y_all)
    original_patches_all = np.array(original_patches_all)

    print(f"總共產生了 {len(X_all)} 個 patches")
    print(f"X_all 形狀: {X_all.shape}")
    print(f"y_all 形狀: {y_all.shape}")
    print(f"original_patches_all 形狀: {original_patches_all.shape}")

    # 重組資料以保持patch的完整性
    patches_per_pair = 56  # 7 × 8 = 56個patches
    num_pairs = len(image_pairs)  
    train_idx = []
    val_idx =[]
    test_idx = []
    
    print(f"重組前 X_all 的形狀: {X_all.shape}")
    print(f"重組前 y_all 的形狀: {y_all.shape}")
    
    # 重組資料為以影像對為單位
    X_reshaped = X_all.reshape(num_pairs, patches_per_pair, 64, 64, 3)
    y_reshaped = y_all.reshape(num_pairs, patches_per_pair, 64, 64)
    # print("X_reshaped shape:", X_reshaped)
    # print("y_reshaped shape:", y_reshaped)

    # 設置隨機種子以便重現結果
    np.random.seed(48)

    # 生成隨機索引
    patch_indices = np.arange(patches_per_pair)
    np.random.shuffle(patch_indices)

    # 計算訓練+驗證集和測試集大小
    train_val_size = int(0.85 * patches_per_pair)  # 85% 訓練+驗證
    test_size = patches_per_pair - train_val_size   # 15% 測試

    # 先分割出測試集
    train_val_idx = patch_indices[:train_val_size]
    test_idx = patch_indices[train_val_size:]

    # 再從訓練+驗證集中分割出驗證集
    val_size = int(0.15 * train_val_size)  # 從訓練集中取 15% 作為驗證集
    train_idx = train_val_idx[:-val_size]
    val_idx = train_val_idx[-val_size:]

    print(f"訓練集索引數量: {len(train_idx)} ({len(train_idx)/patches_per_pair*100:.1f}%)")
    print(f"驗證集索引數量: {len(val_idx)} ({len(val_idx)/patches_per_pair*100:.1f}%)")
    print(f"測試集索引數量: {len(test_idx)} ({len(test_idx)/patches_per_pair*100:.1f}%)")

    # 根據索引分割資料
    X_train = X_reshaped[0][train_idx]
    X_val = X_reshaped[0][val_idx]
    X_test = X_reshaped[0][test_idx]

    y_train = y_reshaped[0][train_idx]
    y_val = y_reshaped[0][val_idx]
    y_test = y_reshaped[0][test_idx]

    print("\n分割後的資料形狀:")
    print(f"X_train shape: {X_train.shape}")
    print(f"X_val shape: {X_val.shape}")
    print(f"X_test shape: {X_test.shape}")
    print(f"y_train shape: {y_train.shape}")
    print(f"y_val shape: {y_val.shape}")
    print(f"y_test shape: {y_test.shape}")
    
    # 儲存處理好的資料
    np.save(os.path.join(output_folder, "X_all.npy"), X_all)
    np.save(os.path.join(output_folder, "y_all.npy"), y_all)
    np.save(os.path.join(output_folder, "original_patches_all.npy"), original_patches_all)
    
    print("資料處理完成並已儲存！")

    # 預處理資料用於U-Net訓練
    X_processed, y_processed = preprocess_data(X_all, y_all)

    # 確保y_processed是二維的（對於二元分類）
    if len(y_processed.shape) == 3:
        y_processed = y_processed.reshape(y_processed.shape[0], -1)

    # 根據之前的索引分割預處理後的資料
    X_train_processed = X_processed[train_idx]
    X_val_processed = X_processed[val_idx]
    X_test_processed = X_processed[test_idx]

    y_train_processed = y_processed[train_idx]
    y_val_processed = y_processed[val_idx]
    y_test_processed = y_processed[test_idx]

    print(f"預處理後 X_train 形狀: {X_train_processed.shape}")
    print(f"預處理後 X_val 形狀: {X_val_processed.shape}")
    print(f"預處理後 X_test 形狀: {X_test_processed.shape}")
    print(f"預處理後 y_train 形狀: {y_train_processed.shape}")
    print(f"預處理後 y_val 形狀: {y_val_processed.shape}")
    print(f"預處理後 y_test 形狀: {y_test_processed.shape}")
    
    # 訓練U-Net模型

    input_channels = X_train.shape[-1] if X_train.shape[-1] > 0 else 3  # 確保波段數至少為3
    print(f"輸入波段數: {input_channels}")
    model = unet_model(input_size=(64, 64, input_channels))

    initial_learning_rate = 0.002  # 降低初始學習率
    min_learning_rate = 0.001  # 避免學習率太低
    warmup_epochs = 16  # 延長 warmup，避免前期過快學習
    total_epochs = 200  # 訓練總次數
    decay_factor = 0.9 # 讓學習率更快下降

    def cosine_decay_with_warmup(epoch):
        if epoch < warmup_epochs:
            return initial_learning_rate * (epoch + 1) / warmup_epochs
        else:
            decay_epochs = (total_epochs - warmup_epochs) * decay_factor
            cosine_decay = 0.5 * (1 + math.cos(math.pi * (epoch - warmup_epochs) / decay_epochs))
            return min_learning_rate + (initial_learning_rate - min_learning_rate) * cosine_decay


    # 定義學習率調度器的回調函數
    lr_callback = tf.keras.callbacks.LearningRateScheduler(cosine_decay_with_warmup)

    # 定義 ModelCheckpoint
    # 使用 .keras 格式的 ModelCheckpoint
    checkpoint = tf.keras.callbacks.ModelCheckpoint(
        'best_model.keras',
        monitor='val_loss',
        save_best_only=True,
        mode='min'
    )

    # 添加額外監控準確度的 ModelCheckpoint
    accuracy_checkpoint = ModelCheckpoint(
        'best_accuracy_model.keras',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1,
        mode='max'
    )

    print("開始訓練U-Net模型...")
    
    # 檢查訓練數據是否為空
    if X_train_processed.size == 0 or y_train_processed.size == 0:
        print("錯誤：訓練數據為空，無法進行模型訓練！")
    else:
        # 訓練模型
        history = model.fit(
            X_train_processed, y_train_processed,
            validation_data=(X_val_processed, y_val_processed),
            batch_size=32,
            epochs=250,
            callbacks=[lr_callback,checkpoint, accuracy_checkpoint],
            verbose=2
        )
        model.summary()

    #Unpatchify
    def process_predictions(y_pred, target_shape=(448, 512)):
        # 首先確保 y_pred 的形狀是 (56, 64, 64, 1) 或 (56, 64, 64)
        if len(y_pred.shape) == 4 and y_pred.shape[-1] == 1:
            y_pred = np.squeeze(y_pred, axis=-1)  # 移除最後一維，變成 (56, 64, 64)
        
        # 確認 y_pred 的形狀是 (56, 64, 64)
        print("處理後的 y_pred 形狀:", y_pred.shape)
        
        # 將 y_pred 重塑為 7x8 的 patch 網格
        patches = y_pred.reshape(7, 8, 64, 64)
        
        # 使用 unpatchify 函數將 patches 重組為完整的預測圖
        full_pred = unpatchify(patches, target_shape)
        
        # 將預測結果二值化
        binary_pred = (full_pred > 0.5).astype(np.uint8)
        
        return binary_pred
    
    X_val = X_reshaped[0].reshape(-1, 64, 64, 3)
    y_val = y_reshaped[0].reshape(-1, 64, 64)

    y_pred = model.predict(X_val)
    print("原始預測形狀:", y_pred.shape)
    # 確保 y_pred 維度正確
    if y_pred.shape[-1] != 1:
        y_pred = np.expand_dims(y_pred, axis=-1)  # 確保最後一維是 1

    full_pred_mask = process_predictions(y_pred)
    print("處理後的預測遮罩形狀:", full_pred_mask.shape)

    # 評估模型
    print("評估模型...")
    test_loss, test_accuracy = model.evaluate(X_test_processed, y_test_processed)
    print(f"測試損失: {test_loss:.4f}")
    print(f"測試準確度: {test_accuracy:.4f}")
    
    # 使用模型進行預測
    print("使用模型進行預測...")
    predictions = model.predict(X_test_processed)
    
    # 將預測結果二值化
    binary_predictions = (predictions > 0.5).astype(np.uint8)

    # 獲取所有 test 樣本的索引位置
    test_locations = [patch_locations[i] for i in test_idx]
    
    full_pred_mask = np.zeros((image_pre.shape[0], image_pre.shape[1]), dtype=np.uint8)

    # 將預測結果填回對應位置
    for i, (row, col) in enumerate(test_locations):
        start_h = row * 64
        start_w = col * 64
        end_h = start_h + 64
        end_w = start_w + 64
        
        # 假設 binary_predictions 的形狀為 (n_samples, height, width, 1)
        # 需要去除最後一個維度
        patch_pred = np.squeeze(binary_predictions[i])
        full_pred_mask[start_h:end_h, start_w:end_w] = patch_pred

    print("視覺化整體結果...")
    # 視覺化結果
    visualize_results(
        original_image=image_pre,
       normalized_post = normalized_post ,
        gsi_image=aligned_gsi,
        label_image=mask,
        pred_image=full_pred_mask,  # 使用 2D mask
        output_folder=output_folder
    )
    print("視覺化整體結果完成")
    print("視覺化分割結果...")
    visualize_multispectral_image(
        original_image=image_pre,
        rgb_corrected_post=normalized_post,
        gsi_image=aligned_gsi,
        label_image=mask,
        pred_image=full_pred_mask,
        output_folder=output_folder
    )
    print("視覺化分割結果完成")

    print("\n計算評估指標...")

    # 1. 計算整體影像的評估指標
    print("\n===== 整體影像評估 =====")
    # 截取有效區域
    true_mask = mask[:target_height, target_width]
    pred_mask = full_pred_mask[:target_height, :target_width]

    # 整體影像二值化
    pred_mask_binary = (pred_mask > 0.5).astype(np.uint8)
    true_mask_binary = (true_mask > 0).astype(np.uint8)

    # 計算整體影像的混淆矩陣元素
    tp_full = np.sum((pred_mask_binary == 1) & (true_mask_binary == 1))
    tn_full = np.sum((pred_mask_binary == 0) & (true_mask_binary == 0))
    fp_full = np.sum((pred_mask_binary == 1) & (true_mask_binary == 0))
    fn_full = np.sum((pred_mask_binary == 0) & (true_mask_binary == 1))

    # 計算整體影像的評估指標
    total_full = tp_full + tn_full + fp_full + fn_full
    accuracy_full = (tp_full + tn_full) / total_full if total_full > 0 else 0

    # 計算整體影像的 IOU
    intersection_full = np.logical_and(true_mask_binary, pred_mask_binary)
    union_full = np.logical_or(true_mask_binary, pred_mask_binary)
    iou_full = np.sum(intersection_full) / np.sum(union_full) if np.sum(union_full) > 0 else 0

    # 1. 可視化整體影像的交集聯集
    plt.figure(figsize=(20, 5))
    plt.suptitle('Full Image', fontsize=16)

    plt.subplot(141)
    plt.imshow(true_mask_binary, cmap='gray')
    plt.title('True Mask')
    plt.axis('off')

    plt.subplot(142)
    plt.imshow(pred_mask_binary, cmap='gray')
    plt.title('Predicted Mask')
    plt.axis('off')

    plt.subplot(143)
    plt.imshow(intersection_full, cmap='gray')
    plt.title(f'Intersection\nValue: {np.sum(intersection_full)}')
    plt.axis('off')

    plt.subplot(144)
    plt.imshow(union_full, cmap='gray')
    plt.title(f'Union\nValue: {np.sum(union_full)}')
    plt.axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(output_folder, 'full_image_iou_visualization.png'))
    plt.close()

    # 輸出評估結果
    print("\n整體影像評估結果:")
    print(f"IOU: {iou_full:.4f}")
    print(f"Accuracy: {accuracy_full:.4f}")
    print(f"TP: {tp_full:.4f}")
    print(f"TN: {tn_full:.4f}")
    print(f"FP: {fp_full:.4f}")
    print(f"FN: {fn_full:.4f}")

    print("\n訓練/驗證/測試影像評估結果:")

    # 初始化變數
    tp_train, tn_train, fp_train, fn_train = 0, 0, 0, 0
    tp_val, tn_val, fp_val, fn_val = 0, 0, 0, 0
    tp_test, tn_test, fp_test, fn_test = 0, 0, 0, 0

    # 影像大小 
    patch_size = 64
    image_height, image_width = true_mask_binary.shape

    # 依據 7x8 的 block 劃分影像
    n_rows, n_cols = 7, 8

    # 計算每個 block 在影像中的範圍
    for idx in train_idx:
        row = idx // n_cols
        col = idx % n_cols
        y_start, y_end = row * patch_size, (row + 1) * patch_size
        x_start, x_end = col * patch_size, (col + 1) * patch_size

        # 確保不超出影像範圍
        y_end = min(y_end, image_height)
        x_end = min(x_end, image_width)

        # 取出該 block 的 Ground Truth 和 預測結果
        true_block = true_mask_binary[y_start:y_end, x_start:x_end]
        pred_block = pred_mask_binary[y_start:y_end, x_start:x_end]

        # 計算 TP, TN, FP, FN
        tp_train += np.sum((pred_block == 1) & (true_block == 1))
        tn_train += np.sum((pred_block == 0) & (true_block == 0))
        fp_train += np.sum((pred_block == 1) & (true_block == 0))
        fn_train += np.sum((pred_block == 0) & (true_block == 1))

    # 驗證集
    for idx in val_idx:
        row = idx // n_cols
        col = idx % n_cols
        y_start, y_end = row * patch_size, (row + 1) * patch_size
        x_start, x_end = col * patch_size, (col + 1) * patch_size
        y_end = min(y_end, image_height)
        x_end = min(x_end, image_width)

        true_block = true_mask_binary[y_start:y_end, x_start:x_end]
        pred_block = pred_mask_binary[y_start:y_end, x_start:x_end]

        tp_val += np.sum((pred_block == 1) & (true_block == 1))
        tn_val += np.sum((pred_block == 0) & (true_block == 0))
        fp_val += np.sum((pred_block == 1) & (true_block == 0))
        fn_val += np.sum((pred_block == 0) & (true_block == 1))

    # 測試集
    for idx in test_idx:
        row = idx // n_cols
        col = idx % n_cols
        y_start, y_end = row * patch_size, (row + 1) * patch_size
        x_start, x_end = col * patch_size, (col + 1) * patch_size
        y_end = min(y_end, image_height)
        x_end = min(x_end, image_width)

        true_block = true_mask_binary[y_start:y_end, x_start:x_end]
        pred_block = pred_mask_binary[y_start:y_end, x_start:x_end]

        tp_test += np.sum((pred_block == 1) & (true_block == 1))
        tn_test += np.sum((pred_block == 0) & (true_block == 0))
        fp_test += np.sum((pred_block == 1) & (true_block == 0))
        fn_test += np.sum((pred_block == 0) & (true_block == 1))

    # 計算評估指標
    def compute_metrics(tp, tn, fp, fn):
        total = tp + tn + fp + fn
        accuracy = (tp + tn) / total if total > 0 else 0
        iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0
        return accuracy, iou

    accuracy_train, iou_train = compute_metrics(tp_train, tn_train, fp_train, fn_train)
    accuracy_val, iou_val = compute_metrics(tp_val, tn_val, fp_val, fn_val)
    accuracy_test, iou_test = compute_metrics(tp_test, tn_test, fp_test, fn_test)

    # 輸出結果
    print(f"訓練集   - IOU: {iou_train:.4f}, Accuracy: {accuracy_train:.4f}, TP: {tp_train}, TN: {tn_train}, FP: {fp_train}, FN: {fn_train}")
    print(f"驗證集   - IOU: {iou_val:.4f}, Accuracy: {accuracy_val:.4f}, TP: {tp_val}, TN: {tn_val}, FP: {fp_val}, FN: {fn_val}")
    print(f"測試集   - IOU: {iou_test:.4f}, Accuracy: {accuracy_test:.4f}, TP: {tp_test}, TN: {tn_test}, FP: {fp_test}, FN: {fn_test}")

    # 從訓練歷史中提取評估指標
    loss = history.history.get('loss', [])
    val_loss = history.history.get('val_loss', [])
    acc = history.history.get('accuracy', [])
    val_acc = history.history.get('val_accuracy', [])
    
    # 將訓練歷史資料轉換為DataFrame格式
    df_history = pd.DataFrame({'loss': loss, 'val_loss': val_loss, 'acc': acc, 'val_acc': val_acc})
    epochs = range(1, len(loss) + 1)
    
    # 繪製損失曲線
    plt.rcParams.update(plt.rcParamsDefault)  # 重設繪圖參數
    # 繪製訓練損失曲線（藍色）
    plt.plot(epochs, loss, 'o-', label='training', markersize=5, color='#4f6b8d')
    # 繪製驗證損失曲線（紅色）
    plt.plot(epochs, val_loss, 'o-', label='validation', markersize=5, color='#cf3832')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend(fontsize='large')  
    plt.grid(linestyle='--')
    plt.tight_layout(pad=1.0)
    plt.show()
    
    # 繪製準確度曲線
    # 繪製訓練準確度曲線（藍色）
    plt.plot(epochs, acc, 'o-', label='training', markersize=5, color='#4f6b8d')
    # 繪製驗證準確度曲線（紅色）
    plt.plot(epochs, val_acc, 'o-', label='validation', markersize=5, color='#cf3832')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(fontsize='large')  # Set the legend font size to large
    plt.grid(linestyle='--')
    plt.tight_layout(pad=1.0)
    plt.show()

    # 繪製訓練歷史
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_folder, 'training_history.png'))
    plt.close()

if __name__ == "__main__":
    main()
